[{"content":"","date":"2026 January 21","externalUrl":null,"permalink":"/tags/architecture/","section":"Tags","summary":"","title":"Architecture","type":"tags"},{"content":" Introduction # The last big project I worked on during my time at Yellowbrick was a brand new observability stack for the core product, an OLAP data warehouse. The company had finished a big push to create a cloud version of the product (which up to that point ran on a custom appliance) and we were finding that our mostly log-driven approach to observability was falling short. Diagnosing performance issues or failures took too much time and effort, and cloud customers had much different expectations on how to observe the health of their systems than our appliance customers.\nThe solution, at a high level, was straightforward: instrument the product to expose Prometheus metrics, and use Grafana to visualize them. Prometheus and Grafana are ubiquitous observability tools and would allow both developers and customers to get quick and meaningful insight into a running system. Reality was more complicated. In broad strokes, the data warehouse consists of three main components: the query planner, the compiler and resource manager, and the execution engine. These components were developed in three different languages, at different times, and by different people. The specifics of how we instrumented them are out of scope for what I want to talk about here, but it took several very talented engineers considerable time and effort to do.\nOne of the core expectations of this project was that we could see all metrics collected on all Yellowbrick systems, customer and internal, in real time. Several posts could be written about how this project as a whole was designed and implemented, but here I want to focus on the metrics transport - that is, how we reliably got metrics from all running Yellowbrick systems into a centralized Thanos deployment, and how we visualized them.\nRequirements # From here, we assume that the product is fully instrumented and exposes useful metrics via a collection of Prometheus-scrapable HTTP endpoints. The requirements for the metrics transport were:\nThe metrics must be available in real time, or close to it. We could not lose data - if our infrastructure was unavailable, metrics needed to queue until transport resumed. We had to be able to remotely configure what metrics are sent to Thanos. Only data from systems with a valid license should be ingested. Support thousands of systems reporting metrics in parallel. Use the same dashboards we shipped to customers; dogfooding ensures they remain useful and polished. Given these requirements, the naive approach of having Prometheus write directly to Thanos breaks down. Prometheus can be configured to write directly to Thanos using RWP (Remote Write Protocol), but using Prometheus\u0026rsquo; RWP functionality alone can\u0026rsquo;t satisfy requirements 2 and 3, and while satisfying requirement 4 is technically possible, doing so is fragile in practice.\nWhen Prometheus writes to a remote endpoint using RWP, it maintains in-memory queues to hold the samples. If the remote endpoint is unavailable for too long, Prometheus eventually starts discarding the oldest samples in the queues. How long \u0026ldquo;too long\u0026rdquo; is will depend on things like how many samples Prometheus is attempting to send and how much memory is available to Prometheus. In practice, this is on the order of minutes or hours, not days, and so satisfying requirement 2 isn\u0026rsquo;t feasible.\nThe fact that Prometheus\u0026rsquo; configuration is static (it\u0026rsquo;s a file on disk) also presents issues. We felt that any approach that required rewriting Prometheus\u0026rsquo; configuration file, be it in response to license changes or changes to what metrics we want to collect, opened us up to too much operational risk. If there was any problem in the configuration, we could not only interrupt the flow of metrics to Thanos, but also prevent Prometheus from coming online at all. For this reason, we could not satisfy requirements 3 and 4 with the naive approach.\nHeimdall # To satisfy all the requirements in an operationally stable way, we developed Heimdall. It comprises two binaries, Heimdall Egress and Heimdall Ingress. I built Heimdall in Golang because I\u0026rsquo;ve found it to be a wonderful programming language for tasks involving concurrency and parallelism (and I just genuinely enjoy working in it). The egress binary runs alongside Prometheus in the customer\u0026rsquo;s Kubernetes cluster and the ingress binary on \u0026ldquo;the mothership\u0026rdquo;. The mothership is a large virtual machine that runs Thanos, Heimdall Ingress, Grafana, and a few other supporting services.\nOverview # The two binaries work together to relay metrics from the customer\u0026rsquo;s Prometheus into Thanos. From the perspective of Prometheus and Thanos the ingress and egress binaries are just services that speak RWP, but to satisfy all our requirements they needed to do a bit more.\nThe egress binary is responsible for:\nRelaying metrics it receives from Prometheus to the ingress binary. Persisting metrics to a disk-based FIFO queue until the ingress binary confirms Thanos has accepted them. Filtering metrics with a whitelist and blacklist regex. Finding and attaching the customer license to each payload sent to the ingress binary. The ingress binary is responsible for:\nRelaying metrics it receives from the egress binary to Thanos. Checking that payloads have a valid license attached. Maintaining a whitelist and blacklist regex per tenant (i.e., customer) that the egress binary can query for. Before we go any further, let\u0026rsquo;s see how data flows through this system. It\u0026rsquo;ll do wonders to keep us on the same page as we dive into the details. The arrows reflect the flow of data.\nPrometheus scrapes the various endpoints exposed by each instance of the instrumented data warehouse and relays them to the egress binary via RWP. The egress binary sends the RWP payload (after filtering it through the whitelist/blacklist) onto the ingress binary, attaching the license (JWT) as an HTTP header. Finally, the ingress binary sends the RWP payload to Thanos if the license is valid, and returns an HTTP 200 to the egress binary to let it know the payload has been received and can be dropped.\nEgress # There are two important flows worth covering in the egress binary: forwarding data from Prometheus to Thanos, and fetching the whitelist and blacklist.\nThe high-level process for receiving and sending RWP payloads looks like this. I omit the details on retrieving a license because it\u0026rsquo;s not very interesting - I just use the Kubernetes API to query for a configmap with a known name (and cache the result to avoid future lookup times).\nWhen metrics come in from Prometheus, they are first fed through a whitelist and blacklist. There are two key reasons we can\u0026rsquo;t simply ship all metrics Prometheus collects to the ingress binary:\nWe can\u0026rsquo;t know the volume of data a customer\u0026rsquo;s Prometheus installation will produce, and there is no good reason to store metrics we are not interested in. Customers might produce metrics with data they wish to keep private. The reason we have both a whitelist and a blacklist (which are regular expressions) is really to accommodate cases where we want all metrics of the form yb_.*, but don\u0026rsquo;t want yb_bad_metric. Without the blacklist we would need to turn that whitelist expression into an explicit list of every metric of that form except yb_bad_metric - this would quickly become a headache.\nTo begin whitelist and blacklist filtering, the egress binary needs to unpack the RWP payload to know the metric names. Any metrics that make it through this filtering are then re-packaged and committed to the FIFO queue. In a separate thread the RWP payloads in the FIFO queue are read and unpacked again. The metric names are then hashed so we can place the data in one of N forwarding queues (which all execute concurrently). Metric names must be deterministically assigned to the same queue because RWP specifies (and Thanos enforces) that data sent for a metric must not have a timestamp older than data already received for that metric. If two samples with the same metric name landed in two different queues, then data could arrive at the ingress binary in the wrong chronological order. When a forwarding queue fills up (or a repeating timeout is hit), the contents of a queue are packaged as an RWP payload and sent to the ingress binary with the customer\u0026rsquo;s license. One might say that we are doing too much work by unpacking the payload twice, but consider:\nTo keep as much disk space available as possible for the metrics we want, it makes sense to filter before putting payloads into the FIFO queue. Queue selection could be done here, but then the number of forwarding queues could not be changed without the risk of sending data in the wrong order. For these reasons, we pay the price of unpacking twice. I had the same concern, but after some testing I found it did not matter at all. The egress binary is mostly network-bound - the only time it used a meaningful amount of CPU was on our internal Kubernetes cluster that had hundreds of data warehouse instances running at any given time.\nThe reason we have more than one forwarding queue is to accommodate bigger deployments. On a typical deployment one forwarding queue is sufficient, but on large deployments or deployments where latency to the mothership was high, we found that the egress binary would struggle to keep up. In some of the worst cases we found the egress binary would fall further and further behind, but these problems disappeared with the parallelism introduced by several forwarding queues.\nMoving on, the ingress binary will send back an acknowledgement or failure. There are three broad cases the egress binary has to deal with:\nPayload accepted: Drop the payload from the FIFO queue. Failure but try again: Wait some time and try again, but don\u0026rsquo;t discard from the FIFO queue. This could occur if the mothership is offline or partially degraded, for example. Failure but don\u0026rsquo;t try again: Drop the payload from the FIFO queue. This could occur if Thanos says the payload is malformed, in which case trying again would not help. The last thing I want to go over on the egress binary is the whitelist and blacklist. The egress binary fetches the whitelist and blacklist from the ingress binary. The egress binary includes the customer license in this request as well so the ingress binary knows which whitelist and blacklist pair to send back, as each tenant can have their own. From the perspective of Heimdall, a tenant is a Kubernetes cluster. The license sent to the ingress binary contains information like customer name, cloud provider, region, and Kubernetes cluster name. Using this information we can derive a unique tenant ID. If the egress binary is unable to get a whitelist and blacklist pair from the ingress binary, a broad default is used for each.\nIngress # Of the two binaries, Heimdall Ingress is the simpler one. It just forwards data to Thanos if the attached license is valid, and maintains a whitelist and blacklist pair for each tenant.\nLicense validation is simple. The ingress binary has a store of public keys and checks to see if the JWT was signed by one of those keys. If the signature is good and the license is not past its expiration date, it\u0026rsquo;s considered valid. We cache successful validations to avoid paying the validation cost each time a payload is received. If an invalid license is detected, the egress binary is told to keep the payload and try again. This is to not lose metrics in cases like a license lapsing and then being renewed the next day. Requests with valid licenses will have their payloads forwarded to Thanos and the reply will be sent back to the egress binary so it can decide what to do with the payload.\nThe thanos-tenant HTTP header in the request to Thanos is set to the tenant ID derived from the customer license. Under the hood, Thanos stores each tenant\u0026rsquo;s data in a separate TSDB (time series database). This is great for addressing any concerns with mixing customer data, and it also means queries for a given tenant only need to search through the data for that tenant.\nWhen the egress binary requests a whitelist and blacklist pair, it also includes a copy of the customer\u0026rsquo;s license. The ingress binary will check to see if a pair exists for the tenant and return it if so. If we haven\u0026rsquo;t set up a specific pair for a tenant, then a default is returned. The whitelist and blacklist pairs are just files on disk with deterministic names. Like many other places in this system, a cache here is used to avoid the cost of repeated lookups.\nThanos # We\u0026rsquo;ve now seen how data arrives at Thanos, but so far I\u0026rsquo;ve abstracted Thanos away as a single entity. In reality, Thanos is a collection of processes that work together. Let\u0026rsquo;s zoom in on the Thanos box we\u0026rsquo;ve seen in the diagrams so far and look at what I actually set up.\nPlumbing # Heimdall Ingress sends RWP payloads to the Thanos Router. The router is a lightweight and stateless process that distributes metrics to the Thanos Ingestors. It\u0026rsquo;s aware of the ingestors via its configuration file and uses a hashing algorithm to determine which ingestor to send any given metric to. As a side note, one should opt for the ketama hashing algorithm as it\u0026rsquo;s a stable hashing algorithm and allows for simple horizontal scaling of Thanos Receivers.\nThe Thanos Ingestors are, in effect, small Prometheus servers. They each maintain a local TSDB and store metrics they receive from the Thanos Router. Every two hours each ingestor uploads a block of data for the last two hours into object storage (AWS S3 in our case). The routers and ingestors are actually the same Thanos process (thanos receive), but take on different responsibilities based on their configuration. It is possible to have one process be both a router and an ingestor, but I chose to break things out into separate processes because the ingestors do significantly more work than the router and that\u0026rsquo;s where I will want to scale. Annoyingly, the documentation for thanos receive does not make it clear at all (at least at the time of writing this) that one could make the process behave as an ingestor only, but I found a blog post on the Thanos website that outlined how to do it.\nThe Thanos Compactor has two responsibilities: to compact blocks uploaded to object storage and to downsample data. How Prometheus blocks work is out of scope for what I want to discuss here, but in general you can assume fewer blocks means less storage used, quicker lookups, and better indices. Downsampling also plays an important role in performance. For example, if one is visualizing a metric over a whole month, it is very unlikely full resolution for that metric is needed - samples with one hour resolution usually do just as well as samples with one minute resolution.\nRetrieving data from the object store is handled by the Thanos Store component. It pulls down the appropriate blocks from object storage to its local disk and executes the query against that data.\nQueries for metrics first land at the Thanos Query component. The result set that Thanos Query sends back for a given query is the union of the result sets from applying that query to the data in each ingestor\u0026rsquo;s local TSDB, and the data in object storage (via Thanos Store). Remember that metrics are sent to different ingestors based on their hash value, so the data on all ingestors must be scanned to produce the correct result set.\nScalability and Performance # The mothership is where all scalability concerns are. The customer-side components (Prometheus and Heimdall Egress) were able to handle the load of our internal Kubernetes cluster that has hundreds of instances running; a typical Yellowbrick deployment will have fewer than ten instances running.\nTo scale the write side of this system, we need to target the router and the ingestors. The ingestors would be the first that need to scale as they do significantly more work than the router. Thankfully, they can scale horizontally and even run on other virtual machines. The router would eventually need to scale horizontally too, but at a much slower pace than the ingestors since the router does so little work (relatively speaking). All of the back pressure in the system is here as well. Each Heimdall Egress binary out in the wild will only have a handful of parallel connections open to the mothership, and they will only send more data as they get back confirmations from Thanos (via Heimdall Ingress). This means the rate at which this system can ingest data will be limited by how well we scale the routers and ingestors, and how fast the ingestors\u0026rsquo; disks are.\nScaling on the read side is affected by how well the Thanos Store component scales, but also how the ingestors scale. Recall that the data we get back for a query comes from both the store component and the ingestors. The store component scales horizontally and can run on different virtual machines. There are a couple of broad ways to scale the store component (hashed vs replica), but we hadn\u0026rsquo;t hit the need for it, so I could only speculate on what would be most appropriate for this system. The read rate is then limited by:\nThe compute power of the ingestor and store components. The disk speed of the ingestor and store components. The bandwidth between the store components and the object storage. We had no real concerns about the bandwidth between Thanos Store and object storage - the mothership is a virtual machine in AWS and we use their S3 service. The compute power and disk speed for the ingestor and store components can be practically scaled out by converting the mothership to be a collection of virtual machines, which presents no real technical challenges, we just haven\u0026rsquo;t needed to go that far yet.\nThe Thanos topology I presented above has so far worked very well for Yellowbrick. Data for thousands of instances is flowing in and engineers are able to observe all corresponding systems in real time. Neither CPU, memory, nor disk are overly taxed on the mothership, which is running on a mid-size virtual machine. This design should scale to meet the organization\u0026rsquo;s needs for years to come. If we were to scale to millions of tenants the system design would need to be reconsidered, but that\u0026rsquo;s a welcome problem of success. Our resources weren\u0026rsquo;t infinite, so we focused on something practical we could get going sooner rather than later, while still reasonably accommodating future growth.\nGrafana # The primary way we expected customers and engineers to interact with the system was via Grafana. The main point of this post is about the metrics pipeline itself, but I did want to touch on Grafana a bit as the dashboards are the most tangible result others would see of our work.\nFrom the very start, we committed to having the dashboards our customers saw be first-class citizens - they needed to be useful and polished. We felt the best way to ensure this was to dogfood them. The dashboards we used internally for engineers, support staff, etc., were the same dashboards we shipped to our customers. To make the dashboards useful when querying across all of our tenants, I created a script that would modify each dashboard to have an extra Grafana variable (i.e., a dropdown at the top of the dashboard) to select the tenant we were interested in. I used a Golang library to parse the PromQL queries for each of the visualizations and add tenant_id=\u0026quot;TENANT_OF_INTEREST\u0026quot; to each query - when this label is set Thanos will only scan the matching TSDB. Other than that, the internal and customer dashboards were functionally and visually equivalent.\nReleasing the dashboards internally was a very rewarding experience. Very quickly I started to get bug reports, requests for new visualizations, and questions about how to add new metrics to the product. This told me others were getting value out of it right away - the best tools are the tools that actually get used.\nConclusion # We set out to solve two broad problems:\nProvide customers with measurable insight and observability into the product. Capture health and diagnostic data for all Yellowbrick systems in near real-time. To do that, we built an observability stack using Prometheus, Thanos, Grafana, and a pair of home-grown binaries called Heimdall. We kept resource overhead at customer sites low, and used battle-tested off-the-shelf software to get the project off the ground as quickly as possible. As a quick summary, the system we created:\nEnforces licensing. Allows us to dynamically select what metrics to collect per tenant. Scales in a predictable way as load increases. Keeps metrics data safe in failure conditions. Provides both customers and engineers with a well-known interface for turning raw metrics into actionable insight. We designed and built this system to scale with the company\u0026rsquo;s needs as we understood them at the time, and it has proven capable of doing so. Startups (and most companies) don\u0026rsquo;t have the luxury of building for every hypothetical future, so we focused on what we actually knew, and built something quickly (but without sacrificing quality).\nIt was a real pleasure to work on this project. I got to touch essentially every part of the core stack, interact with engineers and stakeholders across the whole company, and do some exciting greenfield work. In my opinion, it was one of the most interesting engineering projects happening in the company, and I\u0026rsquo;m thankful I got to be one of the driving forces moving it forward. For those of you that made it this far, thanks for reading!\n","date":"2026 January 21","externalUrl":null,"permalink":"/projects/metrics_pipeline/","section":"Projects","summary":"","title":"Building a Multi-Tenant Metrics Pipeline for Thousands of Clients","type":"projects"},{"content":"","date":"2026 January 21","externalUrl":null,"permalink":"/projects/","section":"Projects","summary":"","title":"Projects","type":"projects"},{"content":"","date":"2026 January 21","externalUrl":null,"permalink":"/tags/software/","section":"Tags","summary":"","title":"Software","type":"tags"},{"content":"","date":"2026 January 21","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"My name is James Robb, and I\u0026rsquo;m looking for a job!\nI\u0026rsquo;m a software engineer, and I pride myself on my ability to deliver what really matters. Building performant, practical, and understandable systems is my thing!\nTo learn about my professional life, please see my CV, LinkedIn, or my post on building a metrics pipeline.\nIf you\u0026rsquo;re interested in what I do outside of work, I also have a bit of content here that focuses on my love for DIY.\n","date":"2026 January 21","externalUrl":null,"permalink":"/","section":"Who Am I?","summary":"","title":"Who Am I?","type":"page"},{"content":"My master\u0026rsquo;s thesis is based on my paper Nested, but Separate: Isolating Unrelated Critical Sections in Real-Time Nested Locking. It provides a much more thorough background on real-time locking to make the material accessible to a wider computer science audience.\nMaster\u0026rsquo;s Thesis PDF\n","date":"2020 December 1","externalUrl":null,"permalink":"/technical_writing/masters_thesis/","section":"Technical Writing","summary":"","title":"Master's Thesis","type":"technical_writing"},{"content":"","date":"2020 December 1","externalUrl":null,"permalink":"/tags/technical-writing/","section":"Tags","summary":"","title":"Technical Writing","type":"tags"},{"content":"","date":"2020 December 1","externalUrl":null,"permalink":"/technical_writing/","section":"Technical Writing","summary":"","title":"Technical Writing","type":"technical_writing"},{"content":"Björn Brandenburg and I introduced a novel fined-grained nested-locking protocol for real-time systems under clustered scheduling. Our protocol provides temporal isolation for latency-sensitive tasks that access (possibly) nested resources while remaining asymptotically optimal. The paper is published in the proceedings of the 32nd Euromicro Conference on Real-Time Systems (ECRTS).\nECRTS Paper PDF\nExtended Technical Report PDF\n","date":"2020 May 1","externalUrl":null,"permalink":"/technical_writing/nested_but_separate/","section":"Technical Writing","summary":"","title":"Nested, but Separate: Isolating Unrelated Critical Sections in Real-Time Nested Locking","type":"technical_writing"},{"content":"Sigurður Helgason and I explore in our bachelor\u0026rsquo;s thesis, Identifying Combinatorial Structures for Binary Strings and Set Partitions, how to build combinatorial specifications for avoidance classes of binary strings and set partitions. Building upon the work of Bean et al. and the CombSpecSearcher system, we show that we can find a combinatorial specification for any avoidance class of binary strings, and combinatorial specifications for many avoidance classes of set partitions.\nI think even in tech and computer science it\u0026rsquo;s unlikely most people know what a combinatorial specification or avoidance class is, so here\u0026rsquo;s a quick motivating example with binary strings.\nLet\u0026rsquo;s say you wanted to know how many binary strings of length 10 do not contain the substring \u0026lsquo;101\u0026rsquo;. This would be easy enough to brute force, but what if you then wanted to know the same thing for binary strings of length 9999? This would be considerably more difficult to brute force. In my thesis I show how to count the number of binary strings of any length that avoid (i.e., do not contain) some given substring. I also explore the same idea applied to set partitions (with plenty of cool tikz graphics!), but you\u0026rsquo;ll have to read the thesis to see how.\nBachelor\u0026rsquo;s Thesis PDF\n","date":"2018 June 13","externalUrl":null,"permalink":"/technical_writing/bachelors_thesis/","section":"Technical Writing","summary":"","title":"Bachelor's Thesis","type":"technical_writing"},{"content":" Let\u0026rsquo;s start with the final product! This is a digital picture frame I built (with my classmate Gunnar) for our Embedded Systems and IoT course at Reykjavik University. It\u0026rsquo;s also going to be my wife\u0026rsquo;s Christmas present, so we made sure to put a lot of extra love and care into it. We used a Raspberry Pi, an LCD panel from a laptop, and some other hobbyist components I\u0026rsquo;ll go over. You connect this picture frame to your home WiFi network to configure it via a web interface, or upload pictures to it with FTP. I have the schematic and all of the code for it on Github .\nHere we can see the auto-dimming feature at work. I emulate a dark room by covering the light sensor. I use increasing exponential decay to calculate the brightness, but it\u0026rsquo;s a bit difficult to truly show that in action. When I performed tests on this feature, I found I liked the way exponential decay worked more than I liked a simple linear relationship between ambient light level and brightness.\nThis is a Raspberry Pi 3 I got as a birthday gift. Always knew I would put it to good use. Thanks Jerome!\nI ordered this guy off of eBay for ~20USD. Its the controller board for the LCD panel I used. I had an extra LCD panel for my laptop laying around after I ordered the wrong one when mine needed replacing. Its expensive to ship things to and from Iceland, so I didn\u0026rsquo;t bother with trying to return it.\nDIY electronics projects always take up way more space, and become way more messy than initially anticipated. This is when things were still relatively sane.\nThe Raspberry PI connects to the controller board with an HDMI cable, and then the controller board connects to the LCD panel with an eDP (embedded display port) ribbon cable. The buttons do what you would expect the button on the side of a monitor to do - open a menu to control brightness, color, etc. The final product doesn\u0026rsquo;t include these buttons.\nI wanted to be able to control the brightness of the display directly from the Raspberry Pi, but since HDMI doesn\u0026rsquo;t support brightness controls, I had to do a bit of extra work. I looked up the data sheet for the controller board and found the PWM (pulse width modulation) pin that controls the brightness of the LCD panel. I took the board over to the electronics lab at my university as they had a microscope we could use. The pins were pretty tiny, so this helped a lot.\nHere you can see we soldered on two wires. One was for enabling the backlight (which I didn\u0026rsquo;t wind up using), and the other was the aforementioned PWM pin.\nAll soldered up!\nThis controller board remembers the settings a user enters with the buttons we saw earlier, even after power has been removed. So what I did was set the brightness to 0 with the buttons and then removed the buttons board. From here on out the Raspberry Pi is in charge of brightness!\nI used a spare piece of acrylic to mount the LCD controller board and Raspberry Pi on. The little guy in the middle is a logic level shifter.\nI use the logic level shifter to convert a 3.3V PWM signal from the Raspberry Pi to a 5V PWM signal that I connect to the PWM brightness control pin on the LCD controller board. The specs I read said the brightness control should be between 0V to 3.3V, but when I measured with my multi-meter I saw 0V to 5V.\nThe controller board takes 12V, but the Raspberry Pi takes 5V. I bought a USB car charger to step down the 12V from the power supply to 5V, and then use a standard micro-USB cable to connect the Raspberry Pi to the power supply.\nThe LCD panel is removed from the laptop screen assembly, and is ready to be put into the frame.\nI had a local frame shop make a frame for me. I like their work, so I keep going back when I need anything frame related.\nI had to carve out a bit of the bottom and side to accommodate the screen. You can see a black piece/stripe along the bottom of the LCD panel in the images above which contains some circuity for the panel.\nLet\u0026rsquo;s call the result rustic. I didn\u0026rsquo;t really have the proper tools on hand to do it properly, and the frame was already assembled when I got it. If I were to do this again, I would make sure this was all carved out before the frame was assembled as it would be way easier to use the proper tools then.\nThe frame also came with a piece of glass. Here I have placed the glass and the LCD panel into the frame and secured them in place with some L-brackets. The extra L-brackets on the left are to hold the power supply.\nThis is the light sensor I used. Its relatively inexpensive and available from Adafruit. I chose it because it has its own ADC (analog to digital circuit), and the Raspberry Pi has no ADC pins. I use the readings from this to add an auto-dimming feature to the picture frame. I created a small program that continually polls this sensor and then adjusts the brightness on the screen accordingly via the PWM pin I interface with on the LCD controller board.\nLight sensor mounted in the recess I carved out for it.\nI had the engineering department at my university use their laser cutter to cut some acrylic discs for me to cover the hole for the light sensor. Originally I had just a pin hole drilled for the light sensor, but the angle at which light could enter was just too narrow. This solution wound up looking pretty cool though, so count me satisfied.\nHere we can see a close up of the acrylic disc covering the light sensor on the top of the frame.\nI used fritzing to whip up a very basic schematic for the project. Unfortunately developing a fritzing component for the LCD controller board was way more work than it was worth, so I just left some notes.\nEverything inside the frame.\nThe digital frame runs a small web server. Once it\u0026rsquo;s connected to the WiFi, you can just pop open your browser and configure what you\u0026rsquo;d like to see and how. Currently the way to upload pictures is via FTP, but this could change in the future if I find the time and the will.\nSo nice.\nI\u0026rsquo;m just having some fun here with my friend\u0026rsquo;s smart phone gimbal.\nI added a last minute \u0026ldquo;seasonal decoration\u0026rdquo; feature. I take an image, in this case a snowflake border, and then superimpose it on each image before it is displayed. Currently I just have one for December.\nThe final product again. Thanks for reading!\n","date":"2017 December 20","externalUrl":null,"permalink":"/projects/auto_dimming_wifi_picture_frame/","section":"Projects","summary":"","title":"Auto-Dimming WiFi-Enabled Digital Picture Frame","type":"projects"},{"content":"","date":"2017 December 20","externalUrl":null,"permalink":"/tags/diy/","section":"Tags","summary":"","title":"DIY","type":"tags"},{"content":"","date":"2017 December 20","externalUrl":null,"permalink":"/tags/electronics/","section":"Tags","summary":"","title":"Electronics","type":"tags"},{"content":"","date":"2017 March 27","externalUrl":null,"permalink":"/tags/open-source/","section":"Tags","summary":"","title":"Open Source","type":"tags"},{"content":"I created a convolutional neural network and a set of complementary utilities for optical character recognition with two classmates in my Introduction to Artificial Intelligence course at Reykjavik University. In particular, I focused on reconstructing water damaged documents written in the Ukrainian language. We used Tensorflow for the neural network and the code is published on GitHub .\nTechnical Writeup PDF\n","date":"2017 March 27","externalUrl":null,"permalink":"/technical_writing/dragnet/","section":"Technical Writing","summary":"","title":"Optical Character Recognition of Cyrillic Characters Using a Convolutional Neural Network","type":"technical_writing"},{"content":"In collaboration with others in the Pirate Party of Iceland, I created Safety Valve. Safety Valve is a platform that allows the residents of Iceland to act as a shadow parliament of sorts. Each bill currently moving through Icelandic parliament is converted into a petition. Residents can then digitally sign for or against each petition using their Ice Key, a digital authentication mechanism provided by the Icelandic government. We ran it for a couple years but it is unfortunately no longer online.\nAll of the code is hosted on Github ","date":"2015 April 9","externalUrl":null,"permalink":"/projects/safety_valve/","section":"Projects","summary":"","title":"Safety Valve (Öryggisventill)","type":"projects"},{"content":"","date":"2015 January 1","externalUrl":null,"permalink":"/tags/icelandic/","section":"Tags","summary":"","title":"Icelandic","type":"tags"},{"content":" Introduction # I built a word clock in the Icelandic Language as a Christmas gift for my wife. I had a ton of fun building it and learned a lot along the way. It was also featured in the local paper.\nThis is the final product. The clock reads \u0026ldquo;klukkan er fimmtán mínútur yfir þrjú\u0026rdquo; which literally translates to \u0026ldquo;the clock is fifteen minutes over three\u0026rdquo; (or said as a native English speaker might \u0026ldquo;the time is fifteen minutes past three\u0026rdquo;). The clock also shows a special birthday message on my wife\u0026rsquo;s birthday - keep reading to see that in action!\nThe design was largely based on a reddit post I saw. While working on the project I had contacted the poster to ask why he had chosen to use two Arduinos. Turns out he was having trouble controlling both the lights and interfacing with the RTC (real-time clock) module. After a bit of chatting we were able to get his design down to just one Arduino.\nI used an Arduino Uno and a breadboard to construct the initial clock. Since then, I have improved upon the design and used EagleCAD to design a custom PCB. In this post I will go through my initial design of the breadboard version and then share some pictures and details about the PCB version. All the code (for both versions) is available on Github .\nBreadboard Version # I started the project by laying out the letters for the clock in Adobe Illustrator. I was able to export an SVG from there and load it into the program that drives the laser cutter. I had never cut anything out with a laser cutter previously but a friend of a friend at the University of Iceland very generously donated his time, expertise, and access to an actual laser cutter to the project.\nWe went through a few pieces of wood before getting the settings right. We had to balance the intensity of the laser, the speed at which it moved, and the thickness of the wood. If the wood was too thick we tended to set it on fire and couldn\u0026rsquo;t get a nice finish, but if the laser moved too quickly or wasn\u0026rsquo;t at a high enough intensity then we wouldn\u0026rsquo;t consistently cut through the wood in all places we wanted to.\nOnce we get the settings right it only took a quick sanding to get the face of the clock looking great. Not seen in this photo, but I did eventually stain the face with a wood stain from the local hardware store.\nOn the left is the wood panel I used to mount all of the LEDs. I had also cut this out with a laser cutter since I had access to it, but it could have just as easily been done with a drill and a bit of patience.\nEach LED has a current limiting resistor. In theory it would have been possible to use one current limiting resistor, but if one does the math it would have needed to be able to dissipate more heat than made sense for a project like this.\nHere I have the \u0026ldquo;brain\u0026rdquo; board mounted on the back of the LED panel. It consists of a few components:\nThe Arduino does all the logic. It will change what words are illuminated, what button presses do when setting the time, etc. The circular bit is an RTC module and keeps track of the time (even when the clock is unplugged). The Arduino\u0026rsquo;s internal clock can drift minutes per day (depending on the quality of the Arduino). The black DIPs are three pairs of shift registers and transistor arrays (located next to each other horizontally). The shift registers are used to expand the number of output pins I have access to (I use one pin per word). The transistor arrays are controlled by the shift registers and drive the LEDs because the shift registers can\u0026rsquo;t reliably handle the current required to power the LEDs directly. I needed a gasket to sit between the LED mounting panel and the face of the clock so that when one word lit up the light didn\u0026rsquo;t bleed into the surrounding letters. I had a local shop cut out a gasket from a thick piece of black acrylic but I was concerned light would still make its way between the acrylic and the LED panel so I tried to use some sticky tack. The sticky tack was not a good solution as it was just too difficult to get a thin and consistent layer around all the words. I wound up using some automotive gasket maker which worked well but I think it was overkill. I found a much better solution which I\u0026rsquo;ll show later in the section about the PCB version of the clock.\nFor LED diffusers I had the same shop cut out the appropriate shapes from a thinner piece of white acrylic and held them in place with glue. This was insanely time consuming and not a solution I would recommend - what I came up with for the PCB version was much less time consuming (and much cheaper).\nGetting to this point was a great feeling - I could finally get a real peek at what the final product would look like.\nHere I have the birthday message \u0026ldquo;til hamingju kata\u0026rdquo; displayed which just means \u0026ldquo;happy birthday kata\u0026rdquo;.\nNext was the frame. This is something I outsourced as well as I just didn\u0026rsquo;t have the tools or space for it, but something I would like to do myself in the future. It\u0026rsquo;s not very complicated, just a very thick picture frame with a notch in it for the LED panel and face to slip into. There is also a small notch for the back of the clock to slip into.\nLike the reddit post I followed I chose to use an translucent acrylic panel for the back of the clock; I think it\u0026rsquo;s really cool to be able to see the insides. If one looks very close they can see the buttons used to set the time wired into the right side of the frame (as seen from the back). There is a mode button and an increment button. The mode button flips between year/month/day/hour/minute (and a birthday demo mode) and the increment button changes the values.\nThe final product as seen from the front one more time. My wife loved it and it was just a blast to build.\nNow, onto the PCB version where I incorporated some of the lessons learned building this version.\nPCB Version # I used EagleCAD to design a custom PCB for this version of the clock. Here I used individually addressable LEDs so now I could control each letter instead of one word at a time, and I could change the colour of each letter instead of just the \u0026ldquo;happy birthday\u0026rdquo; letters. I soldered on all of the SMD LEDs by hand which was time consuming and error prone, but I managed to do it without damaging any of the pads.\nIf I ever made a third version I would make the clock consist of many smaller PCBs (e.g. one for each row) instead of one big board. There are a few reasons I think this would be better:\nI could get a better price on each PCB as I would be ordering more units. When one board was inevitably bad it would mean less waste as it\u0026rsquo;s just a row that would need to be tossed. The solder stencil would be cheaper and I could get away with a relatively small oven. The \u0026ldquo;brain\u0026rdquo; is essentially still just an Arduino. I created a circuit on the back of the PCB for a very simple Arduino and flashed the Arduino bootloader to it. With that I could upload standard Arduino sketches to it. Like the LEDs, all hand soldered!\nThe LEDs I chose use a one-wire protocol and so I only needed one output pin from the embedded Arduino to control all of the LEDs. This meant that I didn\u0026rsquo;t need the shift registers nor the transistor arrays. The downside is that the addressable LEDs were significantly more expensive than just plan white LEDs.\nI simplified the design of the gaskets greatly this time around by using cork. It was cheap, easy to cut, and slightly compressible so I could get a very good seal but pressing all the layers together tight before slipping them into the frame.\nThe diffuser is also much simpler here. Rather than cutting out separate pieces of acrylic I found a plastic film from 3M that is cheap, translucent, and has an adhesive on one side. It doesn\u0026rsquo;t have the shine to it that acrylic does but I found that at a normal viewing distance one could not tell the difference between the plastic film and the white acrylic.\nThis is the final product, which looks pretty much the same as the breadboard version when viewed from the front. I sold this particular clock to someone who lives in the north of Iceland (whose significant other is named Sigrún).\nThanks for reading!\n","date":"2015 January 1","externalUrl":null,"permalink":"/projects/icelandic_word_clock/","section":"Projects","summary":"","title":"Icelandic Word Clock","type":"projects"},{"content":" My father and I purchased a 1977 Kawasaki KZ650 in need of some TLC from some family members in southern Manitoba after I had expressed an interest in motorcycles. My father was a seasoned biker and I learned a lot from him throughout the process. The motorcycle technically ran when we purchased it, but not very well. After a year of learning and a considerable amount of elbow grease, I had a wonderful motorcycle and a great experience under my belt. I replaced parts of the transmission, converted many mechanical components to solid state components, and painted it.\nI unfortunately didn\u0026rsquo;t detail this restoration as well as I would have liked to - it was one of my first big projects.\nDespite my best efforts I could not find a higher resolution photo of the motorcycle on the day I bought it. This is the motorcycle loaded into the back of my beat-up Ford Ranger.\nThe bike ran pretty rough when I got it. One of the first places I started was with the carburetors. I had ordered a new jet and gasket kit online to get them in the best condition I could. I followed the directions to do an initial tuning on them but eventually sought the help of a local mechanic so he could hook them up to a balance bar and sync them.\nThe electrical system was also pretty rough. I found a kit online to convert to a point-less ignition system which was great because I was really bad at shaving down points and I wasn\u0026rsquo;t looking forward to doing it regularly. It worked very well and never gave me problems again.\nThe rectifier/regulator on the bike was an an old mechanical device that would not reliably charge the battery. I ordered a solid state replacement on eBay from Japan. The part arrived in a ziploc-style sandwich bag with instructions printed off on a home printer. The instructions looked like they had gone through Google Translate and I had to read them about twenty times before I could infer what exactly I was supposed to do. The part did work as advertised once installed correctly, though.\nThis bike has both a kick starter and an electric starter. The kick starter worked just fine but the electric starter often wouldn\u0026rsquo;t catch and would instead just grind - it needed a new starter clutch. I went to a local shop, Canadian Motorcycle, that specialized in parts for old motorcycles and to my luck there was another KZ650 sitting in the back. The owner of the shop let me take my tools into the back and pull out the starter clutch. He also gave me a good deal on it since I nicely set aside all of the other parts I took off to get to it.\nWith all of these things replaced the bike ran pretty great. I did some other basic things too like replace the tires and brakes, and had a local reupholstery shop redo the duct tape seat.\nA family friend helped me paint it. Doing this taught me that painting was was all about prep - the actual act of painting was a very small part of the whole process. In hindsight I should have waited for it to warm up a bit (my frowny face is in response to a pretty cold day) but I was just too excited to get the bike finished and on the road.\nI was very happy with the way the painted pieces came out. It\u0026rsquo;s very satisfying to get a glimpse of what the final result will be.\nThis was the result of my efforts. There was definitely more I could have done but I am very happy with it given the budget and resources I had available to me.\nThanks for reading!\n","date":"2010 March 1","externalUrl":null,"permalink":"/projects/kz650/","section":"Projects","summary":"","title":"1977 Kawasaki KZ650 Restoration","type":"projects"},{"content":"","date":"2010 March 1","externalUrl":null,"permalink":"/tags/motorcycles/","section":"Tags","summary":"","title":"Motorcycles","type":"tags"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"My CV and LinkedIn page are largely mirrors of each other.\n\u0026nbsp;PDF\n\u0026nbsp;LinkedIn\n","externalUrl":null,"permalink":"/cv/","section":"Who Am I?","summary":"","title":"Curriculum Vitae","type":"page"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]